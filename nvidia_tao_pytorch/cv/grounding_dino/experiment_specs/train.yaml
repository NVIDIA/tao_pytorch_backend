results_dir: ???
train:
  num_gpus: 1
  num_nodes: 1
  validation_interval: 1
  optim:
    lr_backbone: 2e-05
    lr: 0.0002
    lr_steps: [10, 20]
    momentum: 0.9
  num_epochs: 30
  freeze: ["backbone.0", "bert"]  # if only finetuning
  pretrained_model_path: ???  # if only finetuning
  precision: fp32
dataset:
  train_data_sources:
    - image_dir: ???
      json_file: ???  # COCO-like detection dataset
      label_map: ???
    - image_dir: ???
      json_file: ???  # RefCOCO-like grounding dataset
  val_data_sources:
    image_dir: ???
    json_file: ???  # COCO-like detection dataset (ID must be contiguous and start from 0)
  batch_size: 8
  workers: 16
  max_labels: 80
  dataset_type: serialized  # To reduce the system memory usage
model:
  # pretrained_backbone_path: ???  # uncomment if pretraining
  # text_encoder_type: bert-base-uncased  # set to path if you don't want to download HF checkpoint. uncomment if pretraining
  backbone: swin_tiny_224_1k
  num_feature_levels: 4
  dec_layers: 6
  enc_layers: 6
  num_queries: 900
  dropout_ratio: 0.0
  dim_feedforward: 2048