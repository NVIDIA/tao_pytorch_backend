results_dir: ???
wandb:
  enable: False

train:
  seed: 47
  resume_training_checkpoint_path: null
  num_epochs: 50
  num_nodes: 1
  num_gpus: 1
  gpu_ids: [0]
  validation_interval: 1
  checkpoint_interval: 1

model:
  backbone: resnet_50
  train_backbone: False
  return_interm_indices: [1, 2, 3]
  dec_layers: 6
  enc_layers: 1
  num_queries: 300

quantize:
  backend: modelopt
  mode: static_ptq
  algorithm: max
  model_path:  ???
  results_dir: ${results_dir}
  default_layer_dtype: native
  default_activation_dtype: native

  # Global FP8 E4M3 for all modules (weights + activations)
  layers:
    - module_name: '*'
      weights:
        dtype: fp8_e4m3fn
      activations:
        dtype: fp8_e4m3fn

  # Disable for specific modules/patterns and classes
  skip_names:
    - '*lm_head*'
    - '*block_sparse_moe.gate*'
    - '*router*'
    - '*output_layer*'
    - 'output.*'
    - 'BatchNorm1d'
    - 'BatchNorm2d'
    - 'BatchNorm3d'
    - 'LeakyReLU'
    - '*head*'

dataset:
  # Dataset used exclusively for quantization calibration
  quant_calibration_data_sources:
    image_dir: "???"
    json_file: "???"
  num_classes: ??
  batch_size: 8
  workers: 8
  dataset_type: default
  remap_mscoco_category: True


