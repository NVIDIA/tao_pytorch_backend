encryption_key: tlt_encode
    
results_dir: /tao-pt/tao-experiments/result/nvdinov2
model:
  distill:
    enable: False
    disable_masking: False
    pretrained_non_distill_pl_model_path: null
  backbone: 
    teacher_type: "vit_l"
    student_type: "vit_l"
    drop_path_rate: 0.4
    patch_size: 14
    img_size: 518
  head:
    num_layers: 3
    hidden_dim: 2048
    bottleneck_dim: 384

dataset:
  train_dataset:
    images_dir: /tao-pt/tao-experiments/data/cats_dogs_dataset/training_set/training_set/cats
  batch_size: 16
  workers: 10
  transform:
    n_global_crops: 2
    global_crops_scale: [0.32, 1.0]
    global_crops_size: 224
    n_local_crops: 8
    local_crops_scale: [0.05, 0.32]
    local_crops_size: 98

train:
  resume_training_checkpoint_path: null
  pretrained_model_path: /tao-pt/tao-experiments/pretrained_models/nv_dinov2_classification_model.ckpt
  num_nodes: 1
  num_gpus: 1
  num_epochs: 10
  checkpoint_interval: 1
  layerwise_decay: 1.0
  clip_grad_norm: 3.0
  optim:
    optim: "adamw"
  schedulers:
    learning_rate:
      val_base: "${eval: '2e-4 * (${dataset.batch_size} * ${train.num_gpus} * ${train.num_nodes} / 1024) ** (1/2)'}"
      val_final: 1e-6
      warm_up_steps: 100000
    last_layer_learning_rate:
      val_base: "${eval: '2e-4 * (${dataset.batch_size} * ${train.num_gpus} * ${train.num_nodes} / 1024) ** (1/2)'}"
      val_final: 1e-6
      warm_up_steps: 100000
      freeze_steps: 1250
    weight_decay:
      val_base: 0.04
      val_final: 0.2
    momentum:
      val_base: 0.994
      val_final: 1.0
    teacher_temperature:
      val_base: 0.07
      val_final: 0.07
      val_start: 0.04
      warm_up_steps: 37500
  num_prototypes: 131072
  results_dir: "${results_dir}/train"
